{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cagosx_Otmuw",
        "outputId": "4ff3ab88-c2ab-4be7-ccc2-b7e5d49235a6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import gc\n",
        "import glob\n",
        "import zipfile\n",
        "import shutil\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "\n",
        "\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### PRE-PROCESSING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "##### The generated zips are in name format <Monday_01_25.0.zip> i.e day_week_BW.zip\n",
        "##### Place all zips in root folder and run code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ml-rhhyLFLw"
      },
      "outputs": [],
      "source": [
        "### Specify locations\n",
        "locations = [\"entrance\", \"hall1\", \"hall2\", \"hall3\", \"shop\", \"rest\", \"edge\", \"metaverse\"]\n",
        "\n",
        "\n",
        "##helper functions\n",
        "def map_location(location):\n",
        "    \"\"\"\n",
        "    Maps a given location to its corresponding numeric value.\n",
        "\n",
        "    Args:\n",
        "        location (str): The location to be mapped.\n",
        "\n",
        "    Returns:\n",
        "        int or None: The numeric value corresponding to the location. Returns None if the location is not recognized.\n",
        "\n",
        "    Examples:\n",
        "        >>> map_location(\"entrance\")\n",
        "        2\n",
        "        >>> map_location(\"hall2\")\n",
        "        3\n",
        "        >>> map_location(\"garden\")\n",
        "        None\n",
        "    \"\"\"\n",
        "    location_lower = location.lower()  # Convert location to lowercase\n",
        "    if location_lower == \"entrance\":\n",
        "        return 2\n",
        "    elif location_lower == \"hall1\":\n",
        "        return 1\n",
        "    elif location_lower == \"hall2\":\n",
        "        return 3\n",
        "    elif location_lower == \"hall3\":\n",
        "        return 4\n",
        "    elif location_lower == \"shop\":\n",
        "        return 5\n",
        "    elif location_lower == \"rest\":\n",
        "        return 6\n",
        "    elif location_lower == \"edge\":\n",
        "        return 7\n",
        "    elif location_lower == \"metaverse\":\n",
        "        return 8\n",
        "    else:\n",
        "        return None  # Return None for other cases\n",
        "\n",
        "\n",
        "def assign_value(observation):\n",
        "    split = observation.split('-')\n",
        "    last_part = split[0]\n",
        "    if last_part == 'smartlighting':\n",
        "        return 1\n",
        "    elif last_part == 'visitorguiding':\n",
        "        return 2\n",
        "    elif last_part == 'maintenance':\n",
        "        return 3\n",
        "    elif last_part == 'security':\n",
        "        return 4\n",
        "    else:\n",
        "        return 5  # Return None for cases not specified\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vav1qboXw5pG",
        "outputId": "34335af7-b558-4a3b-9e16-e9d4129ad41f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Zip files list:\n",
            "['./Monday_02_25.0.zip', './Monday_01_3.0.zip', './Monday_02_3.0.zip', './Sunday_02_3.0.zip', './Sunday_02_25.0.zip', './Sunday_01_25.0.zip', './Sunday_01_3.0.zip', './Monday_01_25.0.zip']\n"
          ]
        }
      ],
      "source": [
        "# Define the directory path where you want to search for zip files\n",
        "directory_path = \"./\"\n",
        "\n",
        "# Use glob to find all zip files in the directory\n",
        "zip_files = glob.glob(os.path.join(directory_path, \"*.zip\"))\n",
        "\n",
        "# Save the result in a list\n",
        "zip_file_list = zip_files\n",
        "\n",
        "# Print the list\n",
        "print(\"Zip files list:\")\n",
        "print(zip_file_list)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jcyV_P8mGdwc",
        "outputId": "2099edab-dbe8-4e2e-b349-117b64295b8e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extraction complete for 'Monday_02_25.0'.\n",
            "Folder '/content/extracted/Monday_02_25.0' contains 'Monday'.\n",
            "Day: Monday\n",
            "Week: 2\n",
            "Bandwidth: 25.0\n",
            "Monday_02\n",
            "Renamed results.csv in 'entrance' folder to 'results_entrance_25_pr.csv'\n",
            "Renamed results.csv in 'hall1' folder to 'results_hall1_25_pr.csv'\n",
            "Renamed results.csv in 'hall2' folder to 'results_hall2_25_pr.csv'\n",
            "Renamed results.csv in 'hall3' folder to 'results_hall3_25_pr.csv'\n",
            "Renamed results.csv in 'shop' folder to 'results_shop_25_pr.csv'\n",
            "Renamed results.csv in 'rest' folder to 'results_rest_25_pr.csv'\n",
            "Renamed results.csv in 'edge' folder to 'results_edge_25_pr.csv'\n",
            "Renamed results.csv in 'metaverse' folder to 'results_metaverse_25_pr.csv'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-19-db4cbb662fc0>:171: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(result_file_path, names=columns)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 days 00:02:18.870000\n",
            "Processed file saved to results_entrance_25_mod.csv\n",
            "0 days 00:02:41.271000\n",
            "Processed file saved to results_hall1_25_mod.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-19-db4cbb662fc0>:171: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(result_file_path, names=columns)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 days 00:02:30.260000\n",
            "Processed file saved to results_hall2_25_mod.csv\n",
            "0 days 00:02:27.042000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-19-db4cbb662fc0>:171: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(result_file_path, names=columns)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed file saved to results_hall3_25_mod.csv\n",
            "0 days 00:02:45.187000\n",
            "Processed file saved to results_shop_25_mod.csv\n",
            "0 days 00:02:24.478000\n",
            "Processed file saved to results_rest_25_mod.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-19-db4cbb662fc0>:171: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(result_file_path, names=columns)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 days 00:02:31.887000\n",
            "Processed file saved to results_edge_25_mod.csv\n",
            "0 days 00:02:30.081000\n",
            "Processed file saved to results_metaverse_25_mod.csv\n",
            "Combined CSV file saved at: ./data/Monday_02_25_cres.csv\n",
            "Individual CSV files deleted.\n",
            "Extraction complete for 'Monday_01_3.0'.\n",
            "Folder '/content/extracted/Monday_01_3.0' contains 'Monday'.\n",
            "Day: Monday\n",
            "Week: 1\n",
            "Bandwidth: 3.0\n",
            "Monday_01\n",
            "Renamed results.csv in 'entrance' folder to 'results_entrance_3_pr.csv'\n",
            "Renamed results.csv in 'hall1' folder to 'results_hall1_3_pr.csv'\n",
            "Renamed results.csv in 'hall2' folder to 'results_hall2_3_pr.csv'\n",
            "Renamed results.csv in 'hall3' folder to 'results_hall3_3_pr.csv'\n",
            "Renamed results.csv in 'shop' folder to 'results_shop_3_pr.csv'\n",
            "Renamed results.csv in 'rest' folder to 'results_rest_3_pr.csv'\n",
            "Renamed results.csv in 'edge' folder to 'results_edge_3_pr.csv'\n",
            "Renamed results.csv in 'metaverse' folder to 'results_metaverse_3_pr.csv'\n",
            "0 days 00:01:52.336000\n",
            "Processed file saved to results_entrance_3_mod.csv\n",
            "0 days 00:02:20.339000\n",
            "Processed file saved to results_hall1_3_mod.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-19-db4cbb662fc0>:171: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(result_file_path, names=columns)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 days 00:02:15.158000\n",
            "Processed file saved to results_hall2_3_mod.csv\n",
            "0 days 00:01:41.581000\n",
            "Processed file saved to results_hall3_3_mod.csv\n",
            "0 days 00:02:21.336000\n",
            "Processed file saved to results_shop_3_mod.csv\n",
            "0 days 00:02:17.616000\n",
            "Processed file saved to results_rest_3_mod.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-19-db4cbb662fc0>:171: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(result_file_path, names=columns)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 days 00:02:21.677000\n",
            "Processed file saved to results_edge_3_mod.csv\n",
            "0 days 00:01:29.778000\n",
            "Processed file saved to results_metaverse_3_mod.csv\n",
            "Combined CSV file saved at: ./data/Monday_01_3_cres.csv\n",
            "Individual CSV files deleted.\n",
            "Extraction complete for 'Monday_02_3.0'.\n",
            "Folder '/content/extracted/Monday_02_3.0' contains 'Monday'.\n",
            "Day: Monday\n",
            "Week: 2\n",
            "Bandwidth: 3.0\n",
            "Monday_02\n",
            "Renamed results.csv in 'entrance' folder to 'results_entrance_3_pr.csv'\n",
            "Renamed results.csv in 'hall1' folder to 'results_hall1_3_pr.csv'\n",
            "Renamed results.csv in 'hall2' folder to 'results_hall2_3_pr.csv'\n",
            "Renamed results.csv in 'hall3' folder to 'results_hall3_3_pr.csv'\n",
            "Renamed results.csv in 'shop' folder to 'results_shop_3_pr.csv'\n",
            "Renamed results.csv in 'rest' folder to 'results_rest_3_pr.csv'\n",
            "Renamed results.csv in 'edge' folder to 'results_edge_3_pr.csv'\n",
            "Renamed results.csv in 'metaverse' folder to 'results_metaverse_3_pr.csv'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-19-db4cbb662fc0>:171: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(result_file_path, names=columns)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 days 00:02:17.314000\n",
            "Processed file saved to results_entrance_3_mod.csv\n",
            "0 days 00:02:22.555000\n",
            "Processed file saved to results_hall1_3_mod.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-19-db4cbb662fc0>:171: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(result_file_path, names=columns)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 days 00:02:29.587000\n",
            "Processed file saved to results_hall2_3_mod.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-19-db4cbb662fc0>:171: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(result_file_path, names=columns)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 days 00:02:30.253000\n",
            "Processed file saved to results_hall3_3_mod.csv\n",
            "0 days 00:02:23.075000\n",
            "Processed file saved to results_shop_3_mod.csv\n",
            "0 days 00:02:35.147000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-19-db4cbb662fc0>:171: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(result_file_path, names=columns)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed file saved to results_rest_3_mod.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-19-db4cbb662fc0>:171: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(result_file_path, names=columns)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 days 00:02:20.924000\n",
            "Processed file saved to results_edge_3_mod.csv\n",
            "0 days 00:02:18.745000\n",
            "Processed file saved to results_metaverse_3_mod.csv\n",
            "Combined CSV file saved at: ./data/Monday_02_3_cres.csv\n",
            "Individual CSV files deleted.\n",
            "Extraction complete for 'Sunday_02_3.0'.\n",
            "Folder '/content/extracted/Sunday_02_3.0' contains 'Sunday'.\n",
            "Day: Sunday\n",
            "Week: 2\n",
            "Bandwidth: 3.0\n",
            "Sunday_02\n",
            "Renamed results.csv in 'entrance' folder to 'results_entrance_3_pr.csv'\n",
            "Renamed results.csv in 'hall1' folder to 'results_hall1_3_pr.csv'\n",
            "Renamed results.csv in 'hall2' folder to 'results_hall2_3_pr.csv'\n",
            "Renamed results.csv in 'hall3' folder to 'results_hall3_3_pr.csv'\n",
            "Renamed results.csv in 'shop' folder to 'results_shop_3_pr.csv'\n",
            "Renamed results.csv in 'rest' folder to 'results_rest_3_pr.csv'\n",
            "Renamed results.csv in 'edge' folder to 'results_edge_3_pr.csv'\n",
            "Renamed results.csv in 'metaverse' folder to 'results_metaverse_3_pr.csv'\n",
            "0 days 00:01:41.018000\n",
            "Processed file saved to results_entrance_3_mod.csv\n",
            "0 days 00:02:18.842000\n",
            "Processed file saved to results_hall1_3_mod.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-19-db4cbb662fc0>:171: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(result_file_path, names=columns)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 days 00:02:13.487000\n",
            "Processed file saved to results_hall2_3_mod.csv\n",
            "0 days 00:01:34.541000\n",
            "Processed file saved to results_hall3_3_mod.csv\n",
            "0 days 00:01:43.180000\n",
            "Processed file saved to results_shop_3_mod.csv\n",
            "0 days 00:02:19.458000\n",
            "Processed file saved to results_rest_3_mod.csv\n",
            "0 days 00:02:00.815000\n",
            "Processed file saved to results_edge_3_mod.csv\n",
            "0 days 00:01:30.734000\n",
            "Processed file saved to results_metaverse_3_mod.csv\n",
            "Combined CSV file saved at: ./data/Sunday_02_3_cres.csv\n",
            "Individual CSV files deleted.\n",
            "Extraction complete for 'Sunday_02_25.0'.\n",
            "Folder '/content/extracted/Sunday_02_25.0' contains 'Sunday'.\n",
            "Day: Sunday\n",
            "Week: 2\n",
            "Bandwidth: 25.0\n",
            "Sunday_02\n",
            "Renamed results.csv in 'entrance' folder to 'results_entrance_25_pr.csv'\n",
            "Renamed results.csv in 'hall1' folder to 'results_hall1_25_pr.csv'\n",
            "Renamed results.csv in 'hall2' folder to 'results_hall2_25_pr.csv'\n",
            "Renamed results.csv in 'hall3' folder to 'results_hall3_25_pr.csv'\n",
            "Renamed results.csv in 'shop' folder to 'results_shop_25_pr.csv'\n",
            "Renamed results.csv in 'rest' folder to 'results_rest_25_pr.csv'\n",
            "Renamed results.csv in 'edge' folder to 'results_edge_25_pr.csv'\n",
            "Renamed results.csv in 'metaverse' folder to 'results_metaverse_25_pr.csv'\n",
            "0 days 00:01:44.803000\n",
            "Processed file saved to results_entrance_25_mod.csv\n",
            "0 days 00:01:55.604000\n",
            "Processed file saved to results_hall1_25_mod.csv\n",
            "0 days 00:01:33.708000\n",
            "Processed file saved to results_hall2_25_mod.csv\n",
            "0 days 00:02:05.255000\n",
            "Processed file saved to results_hall3_25_mod.csv\n",
            "0 days 00:02:07.739000\n",
            "Processed file saved to results_shop_25_mod.csv\n",
            "0 days 00:01:38.481000\n",
            "Processed file saved to results_rest_25_mod.csv\n",
            "0 days 00:01:37.369000\n",
            "Processed file saved to results_edge_25_mod.csv\n",
            "0 days 00:01:56.339000\n",
            "Processed file saved to results_metaverse_25_mod.csv\n",
            "Combined CSV file saved at: ./data/Sunday_02_25_cres.csv\n",
            "Individual CSV files deleted.\n",
            "Extraction complete for 'Sunday_01_25.0'.\n",
            "Folder '/content/extracted/Sunday_01_25.0' contains 'Sunday'.\n",
            "Day: Sunday\n",
            "Week: 1\n",
            "Bandwidth: 25.0\n",
            "Sunday_01\n",
            "Renamed results.csv in 'entrance' folder to 'results_entrance_25_pr.csv'\n",
            "Renamed results.csv in 'hall1' folder to 'results_hall1_25_pr.csv'\n",
            "Renamed results.csv in 'hall2' folder to 'results_hall2_25_pr.csv'\n",
            "Renamed results.csv in 'hall3' folder to 'results_hall3_25_pr.csv'\n",
            "Renamed results.csv in 'shop' folder to 'results_shop_25_pr.csv'\n",
            "Renamed results.csv in 'rest' folder to 'results_rest_25_pr.csv'\n",
            "Renamed results.csv in 'edge' folder to 'results_edge_25_pr.csv'\n",
            "Renamed results.csv in 'metaverse' folder to 'results_metaverse_25_pr.csv'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-19-db4cbb662fc0>:171: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(result_file_path, names=columns)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 days 00:02:24.210000\n",
            "Processed file saved to results_entrance_25_mod.csv\n",
            "0 days 00:02:32.783000\n",
            "Processed file saved to results_hall1_25_mod.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-19-db4cbb662fc0>:171: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(result_file_path, names=columns)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 days 00:02:53.157000\n",
            "Processed file saved to results_hall2_25_mod.csv\n",
            "0 days 00:02:20.704000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-19-db4cbb662fc0>:171: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(result_file_path, names=columns)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed file saved to results_hall3_25_mod.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-19-db4cbb662fc0>:171: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(result_file_path, names=columns)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 days 00:02:53.632000\n",
            "Processed file saved to results_shop_25_mod.csv\n",
            "0 days 00:02:53.945000\n",
            "Processed file saved to results_rest_25_mod.csv\n",
            "0 days 00:02:20.090000\n",
            "Processed file saved to results_edge_25_mod.csv\n",
            "0 days 00:02:21.240000\n",
            "Processed file saved to results_metaverse_25_mod.csv\n",
            "Combined CSV file saved at: ./data/Sunday_01_25_cres.csv\n",
            "Individual CSV files deleted.\n",
            "Extraction complete for 'Sunday_01_3.0'.\n",
            "Folder '/content/extracted/Sunday_01_3.0' contains 'Sunday'.\n",
            "Day: Sunday\n",
            "Week: 1\n",
            "Bandwidth: 3.0\n",
            "Sunday_01\n",
            "Renamed results.csv in 'entrance' folder to 'results_entrance_3_pr.csv'\n",
            "Renamed results.csv in 'hall1' folder to 'results_hall1_3_pr.csv'\n",
            "Renamed results.csv in 'hall2' folder to 'results_hall2_3_pr.csv'\n",
            "Renamed results.csv in 'hall3' folder to 'results_hall3_3_pr.csv'\n",
            "Renamed results.csv in 'shop' folder to 'results_shop_3_pr.csv'\n",
            "Renamed results.csv in 'rest' folder to 'results_rest_3_pr.csv'\n",
            "Renamed results.csv in 'edge' folder to 'results_edge_3_pr.csv'\n",
            "Renamed results.csv in 'metaverse' folder to 'results_metaverse_3_pr.csv'\n",
            "0 days 00:01:52.639000\n",
            "Processed file saved to results_entrance_3_mod.csv\n",
            "0 days 00:01:42.309000\n",
            "Processed file saved to results_hall1_3_mod.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-19-db4cbb662fc0>:171: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(result_file_path, names=columns)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 days 00:02:23.062000\n",
            "Processed file saved to results_hall2_3_mod.csv\n",
            "0 days 00:02:23.682000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-19-db4cbb662fc0>:171: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(result_file_path, names=columns)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed file saved to results_hall3_3_mod.csv\n",
            "0 days 00:01:46.625000\n",
            "Processed file saved to results_shop_3_mod.csv\n",
            "0 days 00:01:47.951000\n",
            "Processed file saved to results_rest_3_mod.csv\n",
            "0 days 00:01:40.623000\n",
            "Processed file saved to results_edge_3_mod.csv\n",
            "0 days 00:01:36.326000\n",
            "Processed file saved to results_metaverse_3_mod.csv\n",
            "Combined CSV file saved at: ./data/Sunday_01_3_cres.csv\n",
            "Individual CSV files deleted.\n",
            "Extraction complete for 'Monday_01_25.0'.\n",
            "Folder '/content/extracted/Monday_01_25.0' contains 'Monday'.\n",
            "Day: Monday\n",
            "Week: 1\n",
            "Bandwidth: 25.0\n",
            "Monday_01\n",
            "Renamed results.csv in 'entrance' folder to 'results_entrance_25_pr.csv'\n",
            "Renamed results.csv in 'hall1' folder to 'results_hall1_25_pr.csv'\n",
            "Renamed results.csv in 'hall2' folder to 'results_hall2_25_pr.csv'\n",
            "Renamed results.csv in 'hall3' folder to 'results_hall3_25_pr.csv'\n",
            "Renamed results.csv in 'shop' folder to 'results_shop_25_pr.csv'\n",
            "Renamed results.csv in 'rest' folder to 'results_rest_25_pr.csv'\n",
            "Renamed results.csv in 'edge' folder to 'results_edge_25_pr.csv'\n",
            "Renamed results.csv in 'metaverse' folder to 'results_metaverse_25_pr.csv'\n",
            "0 days 00:02:12.046000\n",
            "Processed file saved to results_entrance_25_mod.csv\n",
            "0 days 00:01:51.505000\n",
            "Processed file saved to results_hall1_25_mod.csv\n",
            "0 days 00:01:33.635000\n",
            "Processed file saved to results_hall2_25_mod.csv\n",
            "0 days 00:02:15.809000\n",
            "Processed file saved to results_hall3_25_mod.csv\n",
            "0 days 00:01:52.620000\n",
            "Processed file saved to results_shop_25_mod.csv\n",
            "0 days 00:01:37.510000\n",
            "Processed file saved to results_rest_25_mod.csv\n",
            "0 days 00:01:37.725000\n",
            "Processed file saved to results_edge_25_mod.csv\n",
            "0 days 00:01:31.184000\n",
            "Processed file saved to results_metaverse_25_mod.csv\n",
            "Combined CSV file saved at: ./data/Monday_01_25_cres.csv\n",
            "Individual CSV files deleted.\n"
          ]
        }
      ],
      "source": [
        "# Define the directory path where you want to search for zip files\n",
        "directory_path = \"./\"\n",
        "\n",
        "# Use glob to find all zip files in the directory\n",
        "zip_files = glob.glob(os.path.join(directory_path, \"*.zip\"))\n",
        "\n",
        "# Loop through each zip file\n",
        "for zip_file_path in zip_files:\n",
        "    # Get the name of the zip file without extension\n",
        "    zip_file_name = os.path.splitext(os.path.basename(zip_file_path))[0]\n",
        "\n",
        "    # Specify the directory where you want to extract the contents\n",
        "    extracted_dir_path = os.path.join('extracted', zip_file_name)\n",
        "\n",
        "    # Check if the directory exists and is not empty\n",
        "    if os.path.exists(extracted_dir_path) and os.listdir(extracted_dir_path):\n",
        "        print(f\"Directory '{extracted_dir_path}' is not empty. Clearing contents...\")\n",
        "        # Forcefully remove the directory and its contents\n",
        "        shutil.rmtree(extracted_dir_path)\n",
        "        print(\"Directory contents cleared.\")\n",
        "\n",
        "    # Create the directory if it doesn't exist\n",
        "    os.makedirs(extracted_dir_path, exist_ok=True)\n",
        "\n",
        "    # Extract the zip file\n",
        "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extracted_dir_path)\n",
        "\n",
        "    print(f\"Extraction complete for '{zip_file_name}'.\")\n",
        "\n",
        "    # Define the list of days of the week\n",
        "    days_of_week = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n",
        "\n",
        "    # Define the base directory\n",
        "    base_directory = \"/content/extracted/\"\n",
        "\n",
        "    # Check if any folder in the base directory contains any day of the week\n",
        "    folder_found = False\n",
        "    for folder_name in os.listdir(base_directory):\n",
        "        for day in days_of_week:\n",
        "            if day in folder_name:\n",
        "                full_path = os.path.join(base_directory, folder_name)\n",
        "                print(f\"Folder '{full_path}' contains '{day}'.\")\n",
        "                folder_found = True\n",
        "                break  # Stop searching for the day once found\n",
        "        if folder_found:\n",
        "            break  # Stop searching if any folder is found\n",
        "\n",
        "    if not folder_found:\n",
        "        print(\"No folder contains any day of the week.\")\n",
        "\n",
        "    # Split the full path to get the folder name\n",
        "    _, folder_name = os.path.split(full_path)\n",
        "\n",
        "    day, week, bw = folder_name.split(\"_\")\n",
        "\n",
        "    # Convert bandwidth to float\n",
        "    week = int(week)\n",
        "    bw = float(bw)\n",
        "\n",
        "    print(\"Day:\", day)\n",
        "    print(\"Week:\", week)\n",
        "    print(\"Bandwidth:\", bw)\n",
        "\n",
        "    day_week = \"_\".join(folder_name.split(\"_\")[:2])\n",
        "    print(day_week)\n",
        "\n",
        "    # Define the full path to the CSV file\n",
        "    csv_file_path = full_path + \"/\" + day_week + \"/visitor_maps.csv\"\n",
        "\n",
        "    # Load the CSV file into a DataFrame\n",
        "    df = pd.read_csv(csv_file_path)\n",
        "\n",
        "    # Add new columns \"day\" and \"week\" to the DataFrame and populate them\n",
        "    df['day'] = day\n",
        "    df['week'] = week\n",
        "    df['Bandwidth'] = bw\n",
        "\n",
        "    bw = int(bw)\n",
        "\n",
        "    # Define a dictionary to map time periods to numerical values\n",
        "    time_period_mapping = {\n",
        "        \"8am to 11am\": 1,\n",
        "        \"11am to 3pm\": 2,\n",
        "        \"3pm to 7pm\": 3\n",
        "    }\n",
        "\n",
        "    # Replace the values in the \"Time Period\" column with numerical values\n",
        "    df['Time Period'] = df['Time Period'].replace(time_period_mapping)\n",
        "\n",
        "    # Define a dictionary to map days of the week to numbers\n",
        "    day_number_mapping = {day: i+1 for i, day in enumerate(days_of_week)}\n",
        "\n",
        "    # Replace the values in the \"day\" column with their corresponding numbers\n",
        "    df['day'] = df['day'].replace(day_number_mapping)\n",
        "\n",
        "    # Define the full path to the CSV file\n",
        "    csv_devices = full_path + \"/\" + day_week + \"/devices.csv\"\n",
        "\n",
        "    # Load the CSV file into a DataFrame\n",
        "    df_2 = pd.read_csv(csv_devices)\n",
        "\n",
        "    # Merge the dataframes on the \"Location\" column\n",
        "    merged_df = pd.merge(df, df_2, on=\"Location\", how=\"inner\")\n",
        "\n",
        "\n",
        "    # Initialize an empty dictionary to store the mappings\n",
        "    nbpeople_mappings_read = {}\n",
        "\n",
        "    # Iterate over the rows of the DataFrame\n",
        "    for index, row in merged_df.iterrows():\n",
        "        # Get the location and visitor count for the current row\n",
        "        location = row['Location']\n",
        "        visitors = row['Visitors']\n",
        "\n",
        "        # Determine the visitor label based on the time period\n",
        "        if row['Time Period'] == 1:\n",
        "            visitor_label = 'visitor_A'\n",
        "        elif row['Time Period'] == 2:\n",
        "            visitor_label = 'visitor_B'\n",
        "        else:\n",
        "            visitor_label = 'visitor_C'\n",
        "\n",
        "        # Update the mappings dictionary\n",
        "        if location not in nbpeople_mappings_read:\n",
        "            nbpeople_mappings_read[location] = {}\n",
        "        nbpeople_mappings_read[location][visitor_label] = visitors\n",
        "\n",
        "\n",
        "    # List of folder names\n",
        "    folders = [\"entrance\", \"hall1\", \"hall2\", \"hall3\", \"shop\", \"rest\", \"edge\", \"metaverse\"]\n",
        "\n",
        "    # fpath = full_path + \"/\"\n",
        "\n",
        "    # Iterate through each folder\n",
        "    for folder_name in folders:\n",
        "        # Define the path to the folder\n",
        "        folder_path = os.path.join(full_path, folder_name)\n",
        "\n",
        "        # Check if the folder exists\n",
        "        if os.path.exists(folder_path):\n",
        "            # Define the path to the results.csv file\n",
        "            results_file_path = os.path.join(folder_path, 'results.csv')\n",
        "\n",
        "            # Check if the results.csv file exists\n",
        "            if os.path.exists(results_file_path):\n",
        "                # Define the new filename\n",
        "                new_filename = f\"results_{folder_name}_{bw}_pr.csv\"\n",
        "\n",
        "                # Rename the results.csv file to the new filename\n",
        "                os.rename(results_file_path, os.path.join(folder_path, new_filename))\n",
        "                print(f\"Renamed results.csv in '{folder_name}' folder to '{new_filename}'\")\n",
        "            else:\n",
        "                print(f\"No results.csv file found in '{folder_name}' folder.\")\n",
        "        else:\n",
        "            print(f\"'{folder_name}' folder does not exist.\")\n",
        "\n",
        "    period = 90\n",
        "\n",
        "    # Define column names\n",
        "    columns = [\"timestamp\", \"sensorid\", \"observation\", \"location\", \"bandwidth\", \"payload\", \"responsetime\"]\n",
        "\n",
        "    # Iterate through each folder\n",
        "    for folder_name, mappings in nbpeople_mappings_read.items():\n",
        "        # Define the path to the result.csv file\n",
        "        result_file_path = os.path.join(full_path, folder_name, f'results_{folder_name}_{bw}_pr.csv')\n",
        "\n",
        "        # Check if the file exists\n",
        "        if os.path.exists(result_file_path):\n",
        "            # Read the CSV file into a DataFrame with specified column names\n",
        "            df = pd.read_csv(result_file_path, names=columns)\n",
        "\n",
        "            # Convert 'timestamp' column to datetime type\n",
        "            df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')\n",
        "\n",
        "            # Sort the DataFrame by 'timestamp'\n",
        "            df.sort_values(by='timestamp', inplace=True)\n",
        "\n",
        "            # Define time intervals\n",
        "            interval_duration = pd.Timedelta(seconds=period)\n",
        "\n",
        "            # Define start time for each interval\n",
        "            start_time_A = df['timestamp'].min()\n",
        "\n",
        "            # print(df['timestamp'].max() - df['timestamp'].min())\n",
        "\n",
        "            start_time_B = start_time_A + interval_duration\n",
        "            start_time_C = start_time_B + interval_duration\n",
        "\n",
        "            # Create 'Time-period' column based on time intervals\n",
        "            df['Time-period'] = 3  # Default value for Time-period 3\n",
        "            df.loc[(df['timestamp'] >= start_time_A) & (df['timestamp'] < start_time_B), 'Time-period'] = 1\n",
        "            df.loc[(df['timestamp'] >= start_time_B) & (df['timestamp'] < start_time_C), 'Time-period'] = 2\n",
        "\n",
        "            # Assign visitor counts based on mappings\n",
        "            df['visitors'] = mappings['visitor_C']  # Default value for visitors\n",
        "            df.loc[(df['timestamp'] >= start_time_A) & (df['timestamp'] < start_time_B), 'visitors'] = mappings['visitor_A']\n",
        "            df.loc[(df['timestamp'] >= start_time_B) & (df['timestamp'] < start_time_C), 'visitors'] = mappings['visitor_B']\n",
        "\n",
        "            # Set 'timestamp' as the index\n",
        "            df.set_index('timestamp', inplace=True)\n",
        "\n",
        "            columns_to_drop = [\"observation\", \"payload\"]\n",
        "            df.drop(columns=columns_to_drop, inplace=True)\n",
        "\n",
        "            df['location'] = df['location'].apply(map_location)\n",
        "            df['sensorid'] = df['sensorid'].apply(assign_value)\n",
        "\n",
        "            # print(df.columns)\n",
        "            df.isnull().values.any()\n",
        "\n",
        "            # Resample data to 20-millisecond intervals and group by 'location' and 'bandwidth'\n",
        "            df_resampled = df.groupby(['sensorid']).resample('10S').mean()\n",
        "\n",
        "            df_resampled = df_resampled.interpolate(method='linear')\n",
        "\n",
        "            ## Uncomment if you want to scale the 'responsetime' column\n",
        "            # scaler = MinMaxScaler()\n",
        "            # df_resampled['responsetime'] = scaler.fit_transform(df_resampled[['responsetime']])\n",
        "\n",
        "            # Assuming 'sensorid' is an important categorical variable\n",
        "            # df = df.groupby('sensorid', group_keys=False).apply(lambda x: x.sample(frac=1.0))\n",
        "\n",
        "            # Save the modified DataFrame to a new CSV file\n",
        "            output_file_path = f'results_{folder_name}_{bw}_mod.csv'\n",
        "            df_resampled.to_csv(output_file_path, index=False)\n",
        "            print(f\"Processed file saved to {output_file_path}\")\n",
        "        else:\n",
        "            print(f\"File not found: {result_file_path}\")\n",
        "\n",
        "    # Get a list of all files ending with '_mod.csv'\n",
        "    output_files = glob.glob(f\"*{bw}_mod.csv\")\n",
        "\n",
        "    # Check if there are any files to concatenate\n",
        "    if output_files:\n",
        "        # Read each CSV file and concatenate into a single DataFrame\n",
        "        combined_df = pd.concat([pd.read_csv(file) for file in output_files], ignore_index=True)\n",
        "\n",
        "        # Save the combined DataFrame to a new CSV file\n",
        "        # Create the directory if it doesn't exist\n",
        "        output_directory = './data/'\n",
        "        os.makedirs(output_directory, exist_ok=True)\n",
        "\n",
        "        merged_df['Location'] = merged_df['Location'].apply(map_location)\n",
        "\n",
        "\n",
        "        # Rename columns in df_2 to match the column names in df_1\n",
        "        merged_df = merged_df.rename(columns={'Time Period': 'Time-period', 'Devices Sum': 'Devices_Sum',\n",
        "                                    'Virtualsensors Sum': 'Virtualsensors_Sum', 'Applications Sum': 'Applications_Sum', 'Location': 'location'})\n",
        "\n",
        "        # Assuming your DataFrame is named final_df\n",
        "\n",
        "        # Convert 'Time-period' column to int\n",
        "        combined_df['Time-period'] = combined_df['Time-period'].astype(int)\n",
        "\n",
        "        # Convert 'location' column to int\n",
        "        combined_df['location'] = combined_df['location'].astype(int)\n",
        "\n",
        "        # Convert 'Time-period' column to int\n",
        "        merged_df['Time-period'] = merged_df['Time-period'].astype(int)\n",
        "\n",
        "        # Convert 'location' column to int\n",
        "        merged_df['location'] = merged_df['location'].astype(int)\n",
        "\n",
        "\n",
        "        # Merge the dataframes based on \"Time-period\" and \"Location\"\n",
        "        final_df = pd.merge(combined_df, merged_df, on=['Time-period', 'location'], how='inner')\n",
        "\n",
        "        final_df = final_df.drop(['bandwidth','visitors'], axis=1)\n",
        "\n",
        "        final_df = final_df.drop_duplicates()\n",
        "\n",
        "        combined_csv_path = f'./data/{day_week}_{bw}_cres.csv'\n",
        "        final_df.to_csv(combined_csv_path, index=False)\n",
        "\n",
        "        print(f'Combined CSV file saved at: {combined_csv_path}')\n",
        "\n",
        "        # Delete individual CSV files\n",
        "        for file in output_files:\n",
        "            os.remove(file)\n",
        "        print('Individual CSV files deleted.')\n",
        "    else:\n",
        "        print('No files ending with \"_mod.csv\" found in the root directory.')\n",
        "\n",
        "    !rm -r /content/extracted\n",
        "\n",
        "    gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JCvucLyGPK9c",
        "outputId": "50f461d7-63e0-4806-d5a2-4bcbd3b5fd6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Combined CSV file saved at: ./data/final_results.csv\n"
          ]
        }
      ],
      "source": [
        "# Get a list of all files ending with 'combined_results.csv'\n",
        "output_files = glob.glob('./data/*_cres.csv')\n",
        "\n",
        "# Initialize an empty list to store DataFrames\n",
        "dfs = []\n",
        "\n",
        "# Loop through each output file\n",
        "for output in output_files:\n",
        "    df = pd.read_csv(output)\n",
        "    dfs.append(df)\n",
        "\n",
        "# Concatenate all DataFrames in the list along rows\n",
        "combined_df = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "# Save the combined DataFrame to a new CSV file\n",
        "output_directory = './data/'\n",
        "combined_csv_path = f'./data/final_results.csv'\n",
        "combined_df.to_csv(combined_csv_path, index=False)\n",
        "\n",
        "print(f'Combined CSV file saved at: {combined_csv_path}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndwMuIFzT1JO",
        "outputId": "d05df092-c9b7-41ef-95ca-1cbc54461344"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CSV file \"/content/data/final_results.csv\" zipped successfully to \"/content/final/final_results.zip\"\n"
          ]
        }
      ],
      "source": [
        "# Define the path to the CSV file\n",
        "csv_file_path = '/content/data/final_results.csv'\n",
        "\n",
        "# Define the directory path for the zip file\n",
        "zip_directory = '/content/final/'\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "os.makedirs(zip_directory, exist_ok=True)\n",
        "\n",
        "# Define the path to the zip file\n",
        "zip_file_path = os.path.join(zip_directory, 'final_results.zip')\n",
        "\n",
        "# Create a zip file and add the CSV file to it\n",
        "with zipfile.ZipFile(zip_file_path, 'w') as zipf:\n",
        "    zipf.write(csv_file_path, arcname=os.path.basename(csv_file_path))\n",
        "\n",
        "print(f'CSV file \"{csv_file_path}\" zipped successfully to \"{zip_file_path}\"')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TrA9C8HFBIaY"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
