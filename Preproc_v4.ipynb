{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "cagosx_Otmuw"
      },
      "outputs": [],
      "source": [
        "#Importing required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import gc\n",
        "import glob\n",
        "import zipfile\n",
        "import shutil\n",
        "\n",
        "\n",
        "# gc.collect()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Specify directories\n",
        "## Run initial Pre-processing for each Bandwidth output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "i8HlbtkHIOTw"
      },
      "outputs": [],
      "source": [
        "# Specify the path to your output zip file\n",
        "zip_file_path = 'BWidth3.zip'\n",
        "\n",
        "# Specify the bandwidth\n",
        "bw = \"bw3\"\n",
        "\n",
        "bandwidth = 3\n",
        "\n",
        "# Specify the path to the directory for visitors mapping\n",
        "visitors_map = 'nbpeople_mappings_bw3.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DONdthT6JP-I",
        "outputId": "c8973bad-b7b7-4cc9-c479-f9ddcf258a56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Directory is not empty. Clearing contents...\n",
            "Directory contents cleared.\n",
            "Extraction complete.\n"
          ]
        }
      ],
      "source": [
        "# Specify the directory where you want to extract the contents\n",
        "extracted_dir_path = 'extracted/'\n",
        "\n",
        "# Check if the directory exists and is not empty\n",
        "if os.path.exists(extracted_dir_path) and os.listdir(extracted_dir_path):\n",
        "    print(\"Directory is not empty. Clearing contents...\")\n",
        "    # Forcefully remove the directory and its contents\n",
        "    shutil.rmtree(extracted_dir_path)\n",
        "    print(\"Directory contents cleared.\")\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "os.makedirs(extracted_dir_path, exist_ok=True)\n",
        "\n",
        "# Extract the zip file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extracted_dir_path)\n",
        "\n",
        "print(\"Extraction complete.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6hxbIyzWeccQ",
        "outputId": "5368cfdd-2f6b-4e7a-ff58-1c2b894b3ef5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Renamed results.csv in 'entrance' folder to 'results_entrance_bw3_pr.csv'\n",
            "Renamed results.csv in 'hall1' folder to 'results_hall1_bw3_pr.csv'\n",
            "Renamed results.csv in 'hall2' folder to 'results_hall2_bw3_pr.csv'\n",
            "Renamed results.csv in 'hall3' folder to 'results_hall3_bw3_pr.csv'\n",
            "Renamed results.csv in 'shop' folder to 'results_shop_bw3_pr.csv'\n",
            "Renamed results.csv in 'rest' folder to 'results_rest_bw3_pr.csv'\n",
            "Renamed results.csv in 'edge' folder to 'results_edge_bw3_pr.csv'\n",
            "Renamed results.csv in 'metaverse' folder to 'results_metaverse_bw3_pr.csv'\n"
          ]
        }
      ],
      "source": [
        "# List of folder names\n",
        "folders = [\"entrance\", \"hall1\", \"hall2\", \"hall3\", \"shop\", \"rest\", \"edge\", \"metaverse\"]\n",
        "\n",
        "# Iterate through each folder\n",
        "for folder_name in folders:\n",
        "    # Define the path to the folder\n",
        "    folder_path = os.path.join('extracted', folder_name)\n",
        "\n",
        "    # Check if the folder exists\n",
        "    if os.path.exists(folder_path):\n",
        "        # Define the path to the results.csv file\n",
        "        results_file_path = os.path.join(folder_path, 'results.csv')\n",
        "\n",
        "        # Check if the results.csv file exists\n",
        "        if os.path.exists(results_file_path):\n",
        "            # Define the new filename\n",
        "            new_filename = f\"results_{folder_name}_{bw}_pr.csv\"\n",
        "\n",
        "            # Rename the results.csv file to the new filename\n",
        "            os.rename(results_file_path, os.path.join(folder_path, new_filename))\n",
        "            print(f\"Renamed results.csv in '{folder_name}' folder to '{new_filename}'\")\n",
        "        else:\n",
        "            print(f\"No results.csv file found in '{folder_name}' folder.\")\n",
        "    else:\n",
        "        print(f\"'{folder_name}' folder does not exist.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NN56eMfkLw3f",
        "outputId": "1fbb0649-a5af-4398-b755-a618e19cd209"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Mappings read from DataFrame:\n",
            "{'entrance': {'visitor_A': 3, 'visitor_B': 2, 'visitor_C': 1}, 'hall1': {'visitor_A': 5, 'visitor_B': 5, 'visitor_C': 5}, 'hall2': {'visitor_A': 4, 'visitor_B': 0, 'visitor_C': 2}, 'hall3': {'visitor_A': 1, 'visitor_B': 7, 'visitor_C': 5}, 'shop': {'visitor_A': 9, 'visitor_B': 4, 'visitor_C': 2}, 'rest': {'visitor_A': 6, 'visitor_B': 5, 'visitor_C': 4}, 'edge': {'visitor_A': 3, 'visitor_B': 2, 'visitor_C': 10}, 'metaverse': {'visitor_A': 11, 'visitor_B': 14, 'visitor_C': 10}}\n"
          ]
        }
      ],
      "source": [
        "# Read the DataFrame from the CSV file\n",
        "df_read = pd.read_csv(visitors_map)\n",
        "\n",
        "# Convert the DataFrame back to the dictionary format\n",
        "nbpeople_mappings_read = df_read.set_index('location').T.to_dict()\n",
        "\n",
        "print(\"\\nMappings read from DataFrame:\")\n",
        "print(nbpeople_mappings_read)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKWgfGACeclo",
        "outputId": "2a9a165d-256b-48f3-e15f-6cfa966629ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 days 00:01:33.519000\n",
            "Processed file saved to results_entrance_bw3_mod.csv\n",
            "0 days 00:01:29.882000\n",
            "Processed file saved to results_hall1_bw3_mod.csv\n",
            "0 days 00:01:33.131000\n",
            "Processed file saved to results_hall2_bw3_mod.csv\n",
            "0 days 00:01:39.349000\n",
            "Processed file saved to results_hall3_bw3_mod.csv\n",
            "0 days 00:01:36.161000\n",
            "Processed file saved to results_shop_bw3_mod.csv\n",
            "0 days 00:01:40.203000\n",
            "Processed file saved to results_rest_bw3_mod.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-96-b2c19b03d546>:14: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(result_file_path, names=columns)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 days 00:02:01.110000\n",
            "Processed file saved to results_edge_bw3_mod.csv\n",
            "0 days 00:01:00.222000\n",
            "Processed file saved to results_metaverse_bw3_mod.csv\n"
          ]
        }
      ],
      "source": [
        "\n",
        "period = 60\n",
        "\n",
        "# Define column names\n",
        "columns = [\"timestamp\", \"sensorid\", \"observation\", \"location\", \"bandwidth\", \"payload\", \"responsetime\"]\n",
        "\n",
        "# Iterate through each folder\n",
        "for folder_name, mappings in nbpeople_mappings_read.items():\n",
        "    # Define the path to the result.csv file\n",
        "    result_file_path = os.path.join('extracted', folder_name, f'results_{folder_name}_{bw}_pr.csv')\n",
        "\n",
        "    # Check if the file exists\n",
        "    if os.path.exists(result_file_path):\n",
        "        # Read the CSV file into a DataFrame with specified column names\n",
        "        df = pd.read_csv(result_file_path, names=columns)\n",
        "\n",
        "        # Assuming df is your DataFrame\n",
        "\n",
        "        # Convert 'timestamp' column to datetime type\n",
        "        df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')\n",
        "        # print(df.head())\n",
        "\n",
        "        # Sort the DataFrame by 'timestamp'\n",
        "        df.sort_values(by='timestamp', inplace=True)\n",
        "\n",
        "        # Calculate the time range\n",
        "        time_range = df['timestamp'].max() - df['timestamp'].min()\n",
        "        print(time_range)\n",
        "\n",
        "        # Define time intervals\n",
        "        ### Time interval as set on the \\system-prototype\\commons\\src\\devices - temp/motion/rfid was 30secs to reduce file size.\n",
        "        interval_duration = pd.Timedelta(seconds=period)\n",
        "\n",
        "        # Define start time for each interval\n",
        "        start_time_A = df['timestamp'].min()\n",
        "        start_time_B = start_time_A + interval_duration\n",
        "        start_time_C = start_time_B + interval_duration\n",
        "\n",
        "        # Assign values based on time intervals\n",
        "        df['visitors'] = mappings['visitor_C']  # Default value C\n",
        "        df.loc[(df['timestamp'] >= start_time_A) & (df['timestamp'] < start_time_B), 'visitors'] = mappings['visitor_A']\n",
        "        df.loc[(df['timestamp'] >= start_time_B) & (df['timestamp'] < start_time_C), 'visitors'] = mappings['visitor_B']\n",
        "\n",
        "\n",
        "        # Change the value of 'bandwidth' column to 6\n",
        "        df['bandwidth'] = bandwidth\n",
        "\n",
        "\n",
        "        # Define the sampling rate for supersampling\n",
        "        # supersampling_rate = '5ms'  # Adjust the sampling rate as needed\n",
        "\n",
        "        # Set the timestamp column as the index\n",
        "        # df.set_index('timestamp', inplace=True)\n",
        "\n",
        "        # Resample the DataFrame to supersample the data\n",
        "        # df = df.resample(supersampling_rate).ffill()\n",
        "\n",
        "        # Define the systematic sampling rate\n",
        "        # n = 30  # Select every 10th data point\n",
        "\n",
        "        # # Choose a random starting point\n",
        "        # random_start = 0  # Start from the first datapoint\n",
        "\n",
        "        # # Perform systematic sampling\n",
        "        # systematic_sample = df.iloc[random_start::n]\n",
        "\n",
        "        # Save the resulting DataFrame to a CSV file\n",
        "        # output_file_path = os.path.join('extracted', folder_name, f'results_{folder_name}_modified.csv')\n",
        "        output_file_path = os.path.join(f'results_{folder_name}_{bw}_mod.csv')\n",
        "        df.to_csv(output_file_path, index=False)\n",
        "        print(f\"Processed file saved to {output_file_path}\")\n",
        "    else:\n",
        "        print(f\"File not found: {result_file_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MuxRheac8S--",
        "outputId": "9710319d-b669-44dc-b816-b4d958c233d5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-97-e3766ee5f9e1>:7: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  combined_df = pd.concat([pd.read_csv(file) for file in output_files], ignore_index=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Combined CSV file saved at: ./data/bw3_combined_results.csv\n"
          ]
        }
      ],
      "source": [
        "# Get a list of all files ending with '_mod.csv'\n",
        "output_files = glob.glob(f\"*{bw}_mod.csv\")\n",
        "\n",
        "# Check if there are any files to concatenate\n",
        "if output_files:\n",
        "    # Read each CSV file and concatenate into a single DataFrame\n",
        "    combined_df = pd.concat([pd.read_csv(file) for file in output_files], ignore_index=True)\n",
        "\n",
        "    # Save the combined DataFrame to a new CSV file\n",
        "    # Create the directory if it doesn't exist\n",
        "    output_directory = './data/'\n",
        "    os.makedirs(output_directory, exist_ok=True)\n",
        "\n",
        "    combined_csv_path = f'./data/{bw}_combined_results.csv'\n",
        "    combined_df.to_csv(combined_csv_path, index=False)\n",
        "\n",
        "    print(f'Combined CSV file saved at: {combined_csv_path}')\n",
        "else:\n",
        "    print('No files ending with \"_mod.csv\" found in the root directory.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "CwQcLZNdHnFA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "0U9aS9mhIXTm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fl1BobQGHnYN"
      },
      "source": [
        "### Run After Obtaining Initial Bandwith combined csv for all locations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "7s5oCxSFAZaG"
      },
      "outputs": [],
      "source": [
        "# Define a function to map location values\n",
        "locations = [\"entrance\", \"hall1\", \"hall2\", \"hall3\", \"shop\", \"rest\", \"edge\", \"metaverse\"]\n",
        "\n",
        "def map_location(location):\n",
        "    location_lower = location.lower()  # Convert location to lowercase\n",
        "    if location_lower == \"entrance\":\n",
        "        return 2\n",
        "    elif location_lower == \"hall1\":\n",
        "        return 1\n",
        "    elif location_lower == \"hall2\":\n",
        "        return 3\n",
        "    elif location_lower == \"hall3\":\n",
        "        return 4\n",
        "    elif location_lower == \"shop\":\n",
        "        return 5\n",
        "    elif location_lower == \"rest\":\n",
        "        return 6\n",
        "    elif location_lower == \"edge\":\n",
        "        return 7\n",
        "    elif location_lower == \"metaverse\":\n",
        "        return 8\n",
        "    else:\n",
        "        return None  # Return None for other cases\n",
        "\n",
        "# Apply the function to the 'location' column\n",
        "# df['location_encoded'] = df['location'].apply(map_location)\n",
        "\n",
        "# print(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "ZnooXwTqAZds"
      },
      "outputs": [],
      "source": [
        "# Define a function to parse and assign values\n",
        "def assign_value(observation):\n",
        "    split = observation.split('-')\n",
        "    last_part = split[0]\n",
        "    if last_part == 'smartlighting':\n",
        "        return 1\n",
        "    elif last_part == 'visitorguiding':\n",
        "        return 2\n",
        "    elif last_part == 'maintenance':\n",
        "        return 3\n",
        "    elif last_part == 'security':\n",
        "        return 4\n",
        "    else:\n",
        "        return 5  # Return None for cases not specified\n",
        "\n",
        "# Apply the function to the 'observation' column\n",
        "# df['sensor'] = df['sensorid'].apply(assign_value)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2yEBsmSnAeb4",
        "outputId": "6d810ec4-9959-44f4-94ab-7c5328a90d6e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-100-95e74415b1af>:9: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(output)\n",
            "<ipython-input-100-95e74415b1af>:9: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(output)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Combined CSV file saved at: ./data/final_results.csv\n"
          ]
        }
      ],
      "source": [
        "# Get a list of all files ending with 'combined_results.csv'\n",
        "output_files = glob.glob('./data/*combined_results.csv')\n",
        "\n",
        "# Initialize an empty DataFrame to store cleaned data\n",
        "cleaned_df = pd.DataFrame()\n",
        "\n",
        "# Loop through each output file\n",
        "for output in output_files:\n",
        "    df = pd.read_csv(output)\n",
        "\n",
        "    # Apply mapping functions\n",
        "    df['location_encoded'] = df['location'].apply(map_location)\n",
        "    df['sensor'] = df['sensorid'].apply(assign_value)\n",
        "\n",
        "    # Select columns to keep\n",
        "    columns_to_keep = [\"bandwidth\", \"responsetime\", \"visitors\", \"sensor\", 'location_encoded']\n",
        "    df_selected = df[columns_to_keep]\n",
        "\n",
        "    # Append the selected DataFrame to the cleaned DataFrame\n",
        "    cleaned_df = pd.concat([cleaned_df, df_selected], ignore_index=True)\n",
        "\n",
        "# Save the combined DataFrame to a new CSV file\n",
        "output_directory = './data/'\n",
        "os.makedirs(output_directory, exist_ok=True)\n",
        "combined_csv_path = f'./data/final_results.csv'\n",
        "cleaned_df.to_csv(combined_csv_path, index=False)\n",
        "\n",
        "print(f'Combined CSV file saved at: {combined_csv_path}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ndwMuIFzT1JO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TrA9C8HFBIaY"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
