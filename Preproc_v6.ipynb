{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOIcSo1fJmqogu94FeGrBJ+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chidiebere-Ogbuchi/TSP_CausalDiscovery/blob/main/Preproc_v6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "cagosx_Otmuw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a48dae30-a2f4-401a-bdd7-fd13f0371ae7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "97"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import gc\n",
        "import glob\n",
        "import zipfile\n",
        "import shutil\n",
        "\n",
        "\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "locations = [\"entrance\", \"hall1\", \"hall2\", \"hall3\", \"shop\", \"rest\", \"edge\", \"metaverse\"]\n",
        "\n",
        "def map_location(location):\n",
        "    location_lower = location.lower()  # Convert location to lowercase\n",
        "    if location_lower == \"entrance\":\n",
        "        return 2\n",
        "    elif location_lower == \"hall1\":\n",
        "        return 1\n",
        "    elif location_lower == \"hall2\":\n",
        "        return 3\n",
        "    elif location_lower == \"hall3\":\n",
        "        return 4\n",
        "    elif location_lower == \"shop\":\n",
        "        return 5\n",
        "    elif location_lower == \"rest\":\n",
        "        return 6\n",
        "    elif location_lower == \"edge\":\n",
        "        return 7\n",
        "    elif location_lower == \"metaverse\":\n",
        "        return 8\n",
        "    else:\n",
        "        return None  # Return None for other cases\n",
        "\n",
        "\n",
        "def assign_value(observation):\n",
        "    split = observation.split('-')\n",
        "    last_part = split[0]\n",
        "    if last_part == 'smartlighting':\n",
        "        return 1\n",
        "    elif last_part == 'visitorguiding':\n",
        "        return 2\n",
        "    elif last_part == 'maintenance':\n",
        "        return 3\n",
        "    elif last_part == 'security':\n",
        "        return 4\n",
        "    else:\n",
        "        return 5  # Return None for cases not specified\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4ml-rhhyLFLw"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the directory path where you want to search for zip files\n",
        "directory_path = \"./\"\n",
        "\n",
        "# Use glob to find all zip files in the directory\n",
        "zip_files = glob.glob(os.path.join(directory_path, \"*.zip\"))\n",
        "\n",
        "# Print the list of zip files\n",
        "print(\"Zip files found in the directory:\")\n",
        "for zip_file in zip_files:\n",
        "    print(zip_file)\n",
        "\n",
        "# Save the result in a list\n",
        "zip_file_list = zip_files\n",
        "\n",
        "# Print the list\n",
        "print(\"Zip files list:\")\n",
        "print(zip_file_list)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vav1qboXw5pG",
        "outputId": "d757805d-5435-4f8f-d44a-9f00ffb01526"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Zip files found in the directory:\n",
            "./Monday_02_3.0.zip\n",
            "./Sunday_01_7.0.zip\n",
            "./Sunday_01_3.0.zip\n",
            "./Monday_01_7.0.zip\n",
            "./Monday_02_7.0.zip\n",
            "./Sunday_02_3.0.zip\n",
            "./Sunday_02_7.0.zip\n",
            "./Monday_01_3.0.zip\n",
            "Zip files list:\n",
            "['./Monday_02_3.0.zip', './Sunday_01_7.0.zip', './Sunday_01_3.0.zip', './Monday_01_7.0.zip', './Monday_02_7.0.zip', './Sunday_02_3.0.zip', './Sunday_02_7.0.zip', './Monday_01_3.0.zip']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the directory path where you want to search for zip files\n",
        "directory_path = \"./\"\n",
        "\n",
        "# Use glob to find all zip files in the directory\n",
        "zip_files = glob.glob(os.path.join(directory_path, \"*.zip\"))\n",
        "\n",
        "# Loop through each zip file\n",
        "for zip_file_path in zip_files:\n",
        "    # Get the name of the zip file without extension\n",
        "    zip_file_name = os.path.splitext(os.path.basename(zip_file_path))[0]\n",
        "\n",
        "    # Specify the directory where you want to extract the contents\n",
        "    extracted_dir_path = os.path.join('extracted', zip_file_name)\n",
        "\n",
        "    # Check if the directory exists and is not empty\n",
        "    if os.path.exists(extracted_dir_path) and os.listdir(extracted_dir_path):\n",
        "        print(f\"Directory '{extracted_dir_path}' is not empty. Clearing contents...\")\n",
        "        # Forcefully remove the directory and its contents\n",
        "        shutil.rmtree(extracted_dir_path)\n",
        "        print(\"Directory contents cleared.\")\n",
        "\n",
        "    # Create the directory if it doesn't exist\n",
        "    os.makedirs(extracted_dir_path, exist_ok=True)\n",
        "\n",
        "    # Extract the zip file\n",
        "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extracted_dir_path)\n",
        "\n",
        "    print(f\"Extraction complete for '{zip_file_name}'.\")\n",
        "\n",
        "    # Define the list of days of the week\n",
        "    days_of_week = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n",
        "\n",
        "    # Define the base directory\n",
        "    base_directory = \"/content/extracted/\"\n",
        "\n",
        "    # Check if any folder in the base directory contains any day of the week\n",
        "    folder_found = False\n",
        "    for folder_name in os.listdir(base_directory):\n",
        "        for day in days_of_week:\n",
        "            if day in folder_name:\n",
        "                full_path = os.path.join(base_directory, folder_name)\n",
        "                print(f\"Folder '{full_path}' contains '{day}'.\")\n",
        "                folder_found = True\n",
        "                break  # Stop searching for the day once found\n",
        "        if folder_found:\n",
        "            break  # Stop searching if any folder is found\n",
        "\n",
        "    if not folder_found:\n",
        "        print(\"No folder contains any day of the week.\")\n",
        "\n",
        "    # Split the full path to get the folder name\n",
        "    _, folder_name = os.path.split(full_path)\n",
        "\n",
        "    day, week, bw = folder_name.split(\"_\")\n",
        "\n",
        "    # Convert bandwidth to float\n",
        "    week = int(week)\n",
        "    bw = float(bw)\n",
        "\n",
        "    print(\"Day:\", day)\n",
        "    print(\"Week:\", week)\n",
        "    print(\"Bandwidth:\", bw)\n",
        "\n",
        "    day_week = \"_\".join(folder_name.split(\"_\")[:2])\n",
        "    print(day_week)\n",
        "\n",
        "    # Define the full path to the CSV file\n",
        "    csv_file_path = full_path + \"/\" + day_week + \"/visitor_maps.csv\"\n",
        "\n",
        "    # Load the CSV file into a DataFrame\n",
        "    df = pd.read_csv(csv_file_path)\n",
        "\n",
        "    # Add new columns \"day\" and \"week\" to the DataFrame and populate them\n",
        "    df['day'] = day\n",
        "    df['week'] = week\n",
        "    df['Bandwidth'] = bw\n",
        "\n",
        "    bw = int(bw)\n",
        "\n",
        "    # Define a dictionary to map time periods to numerical values\n",
        "    time_period_mapping = {\n",
        "        \"8am to 11am\": 1,\n",
        "        \"11am to 3pm\": 2,\n",
        "        \"3pm to 7pm\": 3\n",
        "    }\n",
        "\n",
        "    # Replace the values in the \"Time Period\" column with numerical values\n",
        "    df['Time Period'] = df['Time Period'].replace(time_period_mapping)\n",
        "\n",
        "    # Define a dictionary to map days of the week to numbers\n",
        "    day_number_mapping = {day: i+1 for i, day in enumerate(days_of_week)}\n",
        "\n",
        "    # Replace the values in the \"day\" column with their corresponding numbers\n",
        "    df['day'] = df['day'].replace(day_number_mapping)\n",
        "\n",
        "    # Define the full path to the CSV file\n",
        "    csv_devices = full_path + \"/\" + day_week + \"/devices.csv\"\n",
        "\n",
        "    # Load the CSV file into a DataFrame\n",
        "    df_2 = pd.read_csv(csv_devices)\n",
        "\n",
        "    # Merge the dataframes on the \"Location\" column\n",
        "    merged_df = pd.merge(df, df_2, on=\"Location\", how=\"inner\")\n",
        "\n",
        "\n",
        "    # Initialize an empty dictionary to store the mappings\n",
        "    nbpeople_mappings_read = {}\n",
        "\n",
        "    # Iterate over the rows of the DataFrame\n",
        "    for index, row in merged_df.iterrows():\n",
        "        # Get the location and visitor count for the current row\n",
        "        location = row['Location']\n",
        "        visitors = row['Visitors']\n",
        "\n",
        "        # Determine the visitor label based on the time period\n",
        "        if row['Time Period'] == 1:\n",
        "            visitor_label = 'visitor_A'\n",
        "        elif row['Time Period'] == 2:\n",
        "            visitor_label = 'visitor_B'\n",
        "        else:\n",
        "            visitor_label = 'visitor_C'\n",
        "\n",
        "        # Update the mappings dictionary\n",
        "        if location not in nbpeople_mappings_read:\n",
        "            nbpeople_mappings_read[location] = {}\n",
        "        nbpeople_mappings_read[location][visitor_label] = visitors\n",
        "\n",
        "\n",
        "    # List of folder names\n",
        "    folders = [\"entrance\", \"hall1\", \"hall2\", \"hall3\", \"shop\", \"rest\", \"edge\", \"metaverse\"]\n",
        "\n",
        "    # fpath = full_path + \"/\"\n",
        "\n",
        "    # Iterate through each folder\n",
        "    for folder_name in folders:\n",
        "        # Define the path to the folder\n",
        "        folder_path = os.path.join(full_path, folder_name)\n",
        "\n",
        "        # Check if the folder exists\n",
        "        if os.path.exists(folder_path):\n",
        "            # Define the path to the results.csv file\n",
        "            results_file_path = os.path.join(folder_path, 'results.csv')\n",
        "\n",
        "            # Check if the results.csv file exists\n",
        "            if os.path.exists(results_file_path):\n",
        "                # Define the new filename\n",
        "                new_filename = f\"results_{folder_name}_{bw}_pr.csv\"\n",
        "\n",
        "                # Rename the results.csv file to the new filename\n",
        "                os.rename(results_file_path, os.path.join(folder_path, new_filename))\n",
        "                print(f\"Renamed results.csv in '{folder_name}' folder to '{new_filename}'\")\n",
        "            else:\n",
        "                print(f\"No results.csv file found in '{folder_name}' folder.\")\n",
        "        else:\n",
        "            print(f\"'{folder_name}' folder does not exist.\")\n",
        "\n",
        "    period = 20\n",
        "\n",
        "    # Define column names\n",
        "    columns = [\"timestamp\", \"sensorid\", \"observation\", \"location\", \"bandwidth\", \"payload\", \"responsetime\"]\n",
        "\n",
        "    # Iterate through each folder\n",
        "    for folder_name, mappings in nbpeople_mappings_read.items():\n",
        "        # Define the path to the result.csv file\n",
        "        result_file_path = os.path.join(full_path, folder_name, f'results_{folder_name}_{bw}_pr.csv')\n",
        "\n",
        "        # Check if the file exists\n",
        "        if os.path.exists(result_file_path):\n",
        "            # Read the CSV file into a DataFrame with specified column names\n",
        "            df = pd.read_csv(result_file_path, names=columns)\n",
        "\n",
        "            # Convert 'timestamp' column to datetime type\n",
        "            df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')\n",
        "\n",
        "            # Sort the DataFrame by 'timestamp'\n",
        "            df.sort_values(by='timestamp', inplace=True)\n",
        "\n",
        "            # Define time intervals\n",
        "            interval_duration = pd.Timedelta(seconds=period)\n",
        "\n",
        "            # Define start time for each interval\n",
        "            start_time_A = df['timestamp'].min()\n",
        "            start_time_B = start_time_A + interval_duration\n",
        "            start_time_C = start_time_B + interval_duration\n",
        "\n",
        "            # Create 'Time-period' column based on time intervals\n",
        "            df['Time-period'] = 3  # Default value for Time-period 3\n",
        "            df.loc[(df['timestamp'] >= start_time_A) & (df['timestamp'] < start_time_B), 'Time-period'] = 1\n",
        "            df.loc[(df['timestamp'] >= start_time_B) & (df['timestamp'] < start_time_C), 'Time-period'] = 2\n",
        "\n",
        "            # Assign visitor counts based on mappings\n",
        "            df['visitors'] = mappings['visitor_C']  # Default value for visitors\n",
        "            df.loc[(df['timestamp'] >= start_time_A) & (df['timestamp'] < start_time_B), 'visitors'] = mappings['visitor_A']\n",
        "            df.loc[(df['timestamp'] >= start_time_B) & (df['timestamp'] < start_time_C), 'visitors'] = mappings['visitor_B']\n",
        "\n",
        "            # Assuming 'sensorid' is an important categorical variable\n",
        "            df = df.groupby('sensorid', group_keys=False).apply(lambda x: x.sample(frac=0.75))\n",
        "\n",
        "            # Save the modified DataFrame to a new CSV file\n",
        "            output_file_path = f'results_{folder_name}_{bw}_mod.csv'\n",
        "            df.to_csv(output_file_path, index=False)\n",
        "            print(f\"Processed file saved to {output_file_path}\")\n",
        "        else:\n",
        "            print(f\"File not found: {result_file_path}\")\n",
        "\n",
        "    # Get a list of all files ending with '_mod.csv'\n",
        "    output_files = glob.glob(f\"*{bw}_mod.csv\")\n",
        "\n",
        "    # Check if there are any files to concatenate\n",
        "    if output_files:\n",
        "        # Read each CSV file and concatenate into a single DataFrame\n",
        "        combined_df = pd.concat([pd.read_csv(file) for file in output_files], ignore_index=True)\n",
        "\n",
        "        # Save the combined DataFrame to a new CSV file\n",
        "        # Create the directory if it doesn't exist\n",
        "        output_directory = './data/'\n",
        "        os.makedirs(output_directory, exist_ok=True)\n",
        "\n",
        "        # Convert \"Location\" column in df_1 to lowercase\n",
        "        combined_df['location'] = combined_df['location'].str.lower()\n",
        "\n",
        "        # Rename columns in df_2 to match the column names in df_1\n",
        "        merged_df = merged_df.rename(columns={'Time Period': 'Time-period', 'Devices Sum': 'Devices_Sum',\n",
        "                                    'Virtualsensors Sum': 'Virtualsensors_Sum', 'Applications Sum': 'Applications_Sum', 'Location': 'location'})\n",
        "\n",
        "        # Merge the dataframes based on \"Time-period\" and \"Location\"\n",
        "        final_df = pd.merge(combined_df, merged_df, on=['Time-period', 'location'], how='inner')\n",
        "\n",
        "        # Apply mapping functions\n",
        "        final_df['location_encoded'] = final_df['location'].apply(map_location)\n",
        "        final_df['sensor'] = final_df['sensorid'].apply(assign_value)\n",
        "\n",
        "        final_df = final_df.drop(['timestamp', 'bandwidth', 'payload', 'observation', 'sensorid', 'location', 'visitors'], axis=1)\n",
        "\n",
        "        # Print the merged dataframe\n",
        "        final_df.head()\n",
        "\n",
        "\n",
        "        combined_csv_path = f'./data/{day_week}_{bw}_cres.csv'\n",
        "        final_df.to_csv(combined_csv_path, index=False)\n",
        "\n",
        "        print(f'Combined CSV file saved at: {combined_csv_path}')\n",
        "\n",
        "        # Delete individual CSV files\n",
        "        for file in output_files:\n",
        "            os.remove(file)\n",
        "        print('Individual CSV files deleted.')\n",
        "    else:\n",
        "        print('No files ending with \"_mod.csv\" found in the root directory.')\n",
        "\n",
        "    !rm -r /content/extracted\n",
        "\n",
        "    gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jcyV_P8mGdwc",
        "outputId": "c09e7a31-c1d2-442d-8085-994f85551540"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extraction complete for 'Monday_02_3.0'.\n",
            "Folder '/content/extracted/Monday_02_3.0' contains 'Monday'.\n",
            "Day: Monday\n",
            "Week: 2\n",
            "Bandwidth: 3.0\n",
            "Monday_02\n",
            "Renamed results.csv in 'entrance' folder to 'results_entrance_3_pr.csv'\n",
            "Renamed results.csv in 'hall1' folder to 'results_hall1_3_pr.csv'\n",
            "Renamed results.csv in 'hall2' folder to 'results_hall2_3_pr.csv'\n",
            "Renamed results.csv in 'hall3' folder to 'results_hall3_3_pr.csv'\n",
            "Renamed results.csv in 'shop' folder to 'results_shop_3_pr.csv'\n",
            "Renamed results.csv in 'rest' folder to 'results_rest_3_pr.csv'\n",
            "Renamed results.csv in 'edge' folder to 'results_edge_3_pr.csv'\n",
            "Renamed results.csv in 'metaverse' folder to 'results_metaverse_3_pr.csv'\n",
            "Processed file saved to results_entrance_3_mod.csv\n",
            "Processed file saved to results_hall1_3_mod.csv\n",
            "Processed file saved to results_hall2_3_mod.csv\n",
            "Processed file saved to results_hall3_3_mod.csv\n",
            "Processed file saved to results_shop_3_mod.csv\n",
            "Processed file saved to results_rest_3_mod.csv\n",
            "Processed file saved to results_edge_3_mod.csv\n",
            "Processed file saved to results_metaverse_3_mod.csv\n",
            "Combined CSV file saved at: ./data/Monday_02_3_cres.csv\n",
            "Individual CSV files deleted.\n",
            "Extraction complete for 'Sunday_01_7.0'.\n",
            "Folder '/content/extracted/Sunday_01_7.0' contains 'Sunday'.\n",
            "Day: Sunday\n",
            "Week: 1\n",
            "Bandwidth: 7.0\n",
            "Sunday_01\n",
            "Renamed results.csv in 'entrance' folder to 'results_entrance_7_pr.csv'\n",
            "Renamed results.csv in 'hall1' folder to 'results_hall1_7_pr.csv'\n",
            "Renamed results.csv in 'hall2' folder to 'results_hall2_7_pr.csv'\n",
            "Renamed results.csv in 'hall3' folder to 'results_hall3_7_pr.csv'\n",
            "Renamed results.csv in 'shop' folder to 'results_shop_7_pr.csv'\n",
            "Renamed results.csv in 'rest' folder to 'results_rest_7_pr.csv'\n",
            "Renamed results.csv in 'edge' folder to 'results_edge_7_pr.csv'\n",
            "Renamed results.csv in 'metaverse' folder to 'results_metaverse_7_pr.csv'\n",
            "Processed file saved to results_entrance_7_mod.csv\n",
            "Processed file saved to results_hall1_7_mod.csv\n",
            "Processed file saved to results_hall2_7_mod.csv\n",
            "Processed file saved to results_hall3_7_mod.csv\n",
            "Processed file saved to results_shop_7_mod.csv\n",
            "Processed file saved to results_rest_7_mod.csv\n",
            "Processed file saved to results_edge_7_mod.csv\n",
            "Processed file saved to results_metaverse_7_mod.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-106-ba8e4857e173>:213: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  combined_df = pd.concat([pd.read_csv(file) for file in output_files], ignore_index=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined CSV file saved at: ./data/Sunday_01_7_cres.csv\n",
            "Individual CSV files deleted.\n",
            "Extraction complete for 'Sunday_01_3.0'.\n",
            "Folder '/content/extracted/Sunday_01_3.0' contains 'Sunday'.\n",
            "Day: Sunday\n",
            "Week: 1\n",
            "Bandwidth: 3.0\n",
            "Sunday_01\n",
            "Renamed results.csv in 'entrance' folder to 'results_entrance_3_pr.csv'\n",
            "Renamed results.csv in 'hall1' folder to 'results_hall1_3_pr.csv'\n",
            "Renamed results.csv in 'hall2' folder to 'results_hall2_3_pr.csv'\n",
            "Renamed results.csv in 'hall3' folder to 'results_hall3_3_pr.csv'\n",
            "Renamed results.csv in 'shop' folder to 'results_shop_3_pr.csv'\n",
            "Renamed results.csv in 'rest' folder to 'results_rest_3_pr.csv'\n",
            "Renamed results.csv in 'edge' folder to 'results_edge_3_pr.csv'\n",
            "Renamed results.csv in 'metaverse' folder to 'results_metaverse_3_pr.csv'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-106-ba8e4857e173>:171: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(result_file_path, names=columns)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed file saved to results_entrance_3_mod.csv\n",
            "Processed file saved to results_hall1_3_mod.csv\n",
            "Processed file saved to results_hall2_3_mod.csv\n",
            "Processed file saved to results_hall3_3_mod.csv\n",
            "Processed file saved to results_shop_3_mod.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-106-ba8e4857e173>:171: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(result_file_path, names=columns)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed file saved to results_rest_3_mod.csv\n",
            "Processed file saved to results_edge_3_mod.csv\n",
            "Processed file saved to results_metaverse_3_mod.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-106-ba8e4857e173>:213: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  combined_df = pd.concat([pd.read_csv(file) for file in output_files], ignore_index=True)\n",
            "<ipython-input-106-ba8e4857e173>:213: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  combined_df = pd.concat([pd.read_csv(file) for file in output_files], ignore_index=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined CSV file saved at: ./data/Sunday_01_3_cres.csv\n",
            "Individual CSV files deleted.\n",
            "Extraction complete for 'Monday_01_7.0'.\n",
            "Folder '/content/extracted/Monday_01_7.0' contains 'Monday'.\n",
            "Day: Monday\n",
            "Week: 1\n",
            "Bandwidth: 7.0\n",
            "Monday_01\n",
            "Renamed results.csv in 'entrance' folder to 'results_entrance_7_pr.csv'\n",
            "Renamed results.csv in 'hall1' folder to 'results_hall1_7_pr.csv'\n",
            "Renamed results.csv in 'hall2' folder to 'results_hall2_7_pr.csv'\n",
            "Renamed results.csv in 'hall3' folder to 'results_hall3_7_pr.csv'\n",
            "Renamed results.csv in 'shop' folder to 'results_shop_7_pr.csv'\n",
            "Renamed results.csv in 'rest' folder to 'results_rest_7_pr.csv'\n",
            "Renamed results.csv in 'edge' folder to 'results_edge_7_pr.csv'\n",
            "Renamed results.csv in 'metaverse' folder to 'results_metaverse_7_pr.csv'\n",
            "Processed file saved to results_entrance_7_mod.csv\n",
            "Processed file saved to results_hall1_7_mod.csv\n",
            "Processed file saved to results_hall2_7_mod.csv\n",
            "Processed file saved to results_hall3_7_mod.csv\n",
            "Processed file saved to results_shop_7_mod.csv\n",
            "Processed file saved to results_rest_7_mod.csv\n",
            "Processed file saved to results_edge_7_mod.csv\n",
            "Processed file saved to results_metaverse_7_mod.csv\n",
            "Combined CSV file saved at: ./data/Monday_01_7_cres.csv\n",
            "Individual CSV files deleted.\n",
            "Extraction complete for 'Monday_02_7.0'.\n",
            "Folder '/content/extracted/Monday_02_7.0' contains 'Monday'.\n",
            "Day: Monday\n",
            "Week: 2\n",
            "Bandwidth: 7.0\n",
            "Monday_02\n",
            "Renamed results.csv in 'entrance' folder to 'results_entrance_7_pr.csv'\n",
            "Renamed results.csv in 'hall1' folder to 'results_hall1_7_pr.csv'\n",
            "Renamed results.csv in 'hall2' folder to 'results_hall2_7_pr.csv'\n",
            "Renamed results.csv in 'hall3' folder to 'results_hall3_7_pr.csv'\n",
            "Renamed results.csv in 'shop' folder to 'results_shop_7_pr.csv'\n",
            "Renamed results.csv in 'rest' folder to 'results_rest_7_pr.csv'\n",
            "Renamed results.csv in 'edge' folder to 'results_edge_7_pr.csv'\n",
            "Renamed results.csv in 'metaverse' folder to 'results_metaverse_7_pr.csv'\n",
            "Processed file saved to results_entrance_7_mod.csv\n",
            "Processed file saved to results_hall1_7_mod.csv\n",
            "Processed file saved to results_hall2_7_mod.csv\n",
            "Processed file saved to results_hall3_7_mod.csv\n",
            "Processed file saved to results_shop_7_mod.csv\n",
            "Processed file saved to results_rest_7_mod.csv\n",
            "Processed file saved to results_edge_7_mod.csv\n",
            "Processed file saved to results_metaverse_7_mod.csv\n",
            "Combined CSV file saved at: ./data/Monday_02_7_cres.csv\n",
            "Individual CSV files deleted.\n",
            "Extraction complete for 'Sunday_02_3.0'.\n",
            "Folder '/content/extracted/Sunday_02_3.0' contains 'Sunday'.\n",
            "Day: Sunday\n",
            "Week: 2\n",
            "Bandwidth: 3.0\n",
            "Sunday_02\n",
            "Renamed results.csv in 'entrance' folder to 'results_entrance_3_pr.csv'\n",
            "Renamed results.csv in 'hall1' folder to 'results_hall1_3_pr.csv'\n",
            "Renamed results.csv in 'hall2' folder to 'results_hall2_3_pr.csv'\n",
            "Renamed results.csv in 'hall3' folder to 'results_hall3_3_pr.csv'\n",
            "Renamed results.csv in 'shop' folder to 'results_shop_3_pr.csv'\n",
            "No results.csv file found in 'rest' folder.\n",
            "Renamed results.csv in 'edge' folder to 'results_edge_3_pr.csv'\n",
            "Renamed results.csv in 'metaverse' folder to 'results_metaverse_3_pr.csv'\n",
            "Processed file saved to results_entrance_3_mod.csv\n",
            "Processed file saved to results_hall1_3_mod.csv\n",
            "Processed file saved to results_hall2_3_mod.csv\n",
            "Processed file saved to results_hall3_3_mod.csv\n",
            "Processed file saved to results_shop_3_mod.csv\n",
            "File not found: /content/extracted/Sunday_02_3.0/rest/results_rest_3_pr.csv\n",
            "Processed file saved to results_edge_3_mod.csv\n",
            "Processed file saved to results_metaverse_3_mod.csv\n",
            "Combined CSV file saved at: ./data/Sunday_02_3_cres.csv\n",
            "Individual CSV files deleted.\n",
            "Extraction complete for 'Sunday_02_7.0'.\n",
            "Folder '/content/extracted/Sunday_02_7.0' contains 'Sunday'.\n",
            "Day: Sunday\n",
            "Week: 2\n",
            "Bandwidth: 7.0\n",
            "Sunday_02\n",
            "Renamed results.csv in 'entrance' folder to 'results_entrance_7_pr.csv'\n",
            "Renamed results.csv in 'hall1' folder to 'results_hall1_7_pr.csv'\n",
            "Renamed results.csv in 'hall2' folder to 'results_hall2_7_pr.csv'\n",
            "Renamed results.csv in 'hall3' folder to 'results_hall3_7_pr.csv'\n",
            "Renamed results.csv in 'shop' folder to 'results_shop_7_pr.csv'\n",
            "Renamed results.csv in 'rest' folder to 'results_rest_7_pr.csv'\n",
            "Renamed results.csv in 'edge' folder to 'results_edge_7_pr.csv'\n",
            "Renamed results.csv in 'metaverse' folder to 'results_metaverse_7_pr.csv'\n",
            "Processed file saved to results_entrance_7_mod.csv\n",
            "Processed file saved to results_hall1_7_mod.csv\n",
            "Processed file saved to results_hall2_7_mod.csv\n",
            "Processed file saved to results_hall3_7_mod.csv\n",
            "Processed file saved to results_shop_7_mod.csv\n",
            "Processed file saved to results_rest_7_mod.csv\n",
            "Processed file saved to results_edge_7_mod.csv\n",
            "Processed file saved to results_metaverse_7_mod.csv\n",
            "Combined CSV file saved at: ./data/Sunday_02_7_cres.csv\n",
            "Individual CSV files deleted.\n",
            "Extraction complete for 'Monday_01_3.0'.\n",
            "Folder '/content/extracted/Monday_01_3.0' contains 'Monday'.\n",
            "Day: Monday\n",
            "Week: 1\n",
            "Bandwidth: 3.0\n",
            "Monday_01\n",
            "Renamed results.csv in 'entrance' folder to 'results_entrance_3_pr.csv'\n",
            "Renamed results.csv in 'hall1' folder to 'results_hall1_3_pr.csv'\n",
            "Renamed results.csv in 'hall2' folder to 'results_hall2_3_pr.csv'\n",
            "Renamed results.csv in 'hall3' folder to 'results_hall3_3_pr.csv'\n",
            "Renamed results.csv in 'shop' folder to 'results_shop_3_pr.csv'\n",
            "Renamed results.csv in 'rest' folder to 'results_rest_3_pr.csv'\n",
            "Renamed results.csv in 'edge' folder to 'results_edge_3_pr.csv'\n",
            "Renamed results.csv in 'metaverse' folder to 'results_metaverse_3_pr.csv'\n",
            "Processed file saved to results_entrance_3_mod.csv\n",
            "Processed file saved to results_hall1_3_mod.csv\n",
            "Processed file saved to results_hall2_3_mod.csv\n",
            "Processed file saved to results_hall3_3_mod.csv\n",
            "Processed file saved to results_shop_3_mod.csv\n",
            "Processed file saved to results_rest_3_mod.csv\n",
            "Processed file saved to results_edge_3_mod.csv\n",
            "Processed file saved to results_metaverse_3_mod.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-106-ba8e4857e173>:213: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  combined_df = pd.concat([pd.read_csv(file) for file in output_files], ignore_index=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined CSV file saved at: ./data/Monday_01_3_cres.csv\n",
            "Individual CSV files deleted.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Get a list of all files ending with 'combined_results.csv'\n",
        "output_files = glob.glob('./data/*_cres.csv')\n",
        "\n",
        "# Initialize an empty list to store DataFrames\n",
        "dfs = []\n",
        "\n",
        "# Loop through each output file\n",
        "for output in output_files:\n",
        "    df = pd.read_csv(output)\n",
        "    dfs.append(df)\n",
        "\n",
        "# Concatenate all DataFrames in the list along rows\n",
        "combined_df = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "# Save the combined DataFrame to a new CSV file\n",
        "output_directory = './data/'\n",
        "combined_csv_path = f'./data/final_results.csv'\n",
        "combined_df.to_csv(combined_csv_path, index=False)\n",
        "\n",
        "print(f'Combined CSV file saved at: {combined_csv_path}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JCvucLyGPK9c",
        "outputId": "074034a8-0510-4d05-82d4-402a57dc8769"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined CSV file saved at: ./data/final_results.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path to the CSV file\n",
        "csv_file_path = '/content/data/final_results.csv'\n",
        "\n",
        "# Define the directory path for the zip file\n",
        "zip_directory = '/content/final/'\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "os.makedirs(zip_directory, exist_ok=True)\n",
        "\n",
        "# Define the path to the zip file\n",
        "zip_file_path = os.path.join(zip_directory, 'final_results.zip')\n",
        "\n",
        "# Create a zip file and add the CSV file to it\n",
        "with zipfile.ZipFile(zip_file_path, 'w') as zipf:\n",
        "    zipf.write(csv_file_path, arcname=os.path.basename(csv_file_path))\n",
        "\n",
        "print(f'CSV file \"{csv_file_path}\" zipped successfully to \"{zip_file_path}\"')\n"
      ],
      "metadata": {
        "id": "ndwMuIFzT1JO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0846d15-a208-42dd-cabe-28d3e883efd4"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV file \"/content/data/final_results.csv\" zipped successfully to \"/content/final/final_results.zip\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TrA9C8HFBIaY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}